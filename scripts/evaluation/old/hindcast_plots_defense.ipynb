{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'deeps2aEnv (Python 3.12.2)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import io, math\n",
    "import numpy as np\n",
    "import zarr\n",
    "import xarray as xr\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "\n",
    "from s2aenso.utils import data, normalization, metric\n",
    "from s2aenso.utils.utilities import *\n",
    "from s2aenso.model import swinlstm, losses\n",
    "\n",
    "#(839072, \"s12\", \"s12\")\n",
    "#(815035, \"7var_stauxy\", \"7_level_\" + r'$s\\tau_xy$')\n",
    "model_num_swin = 839072\n",
    "model_num_vit = 815035\n",
    "\n",
    "num_subsamples = 10\n",
    "\n",
    "dataset = \"godas\" # \"cesm2_picontrol\" or \"oras5\" or \"cesm2_lens\"\n",
    "model = \"swinlstm\" # \"swinlstm\" or \"vit\"\n",
    "run_specification = \"s12\"\n",
    "run_specification2 = \"s12\" \n",
    "three_month_mean = True\n",
    "\n",
    "model_num = model_num_vit if model == \"vit\" else model_num_swin\n",
    "\n",
    "\n",
    "PATH = f\"C:/Users/felix/PycharmProjects/deeps2a-enso/scripts/evaluation/results/unprocessed/raw_preds_targs/{model}_{model_num}_{run_specification}/\"\n",
    "save_dir_ = f\"C:/Users/felix/PycharmProjects/deeps2a-enso/scripts/evaluation/results/hindcasts/{dataset}/{model}_{model_num}_{run_specification2}/\"\n",
    "create_directory(save_dir_)\n",
    "\n",
    "\n",
    "pred_path = PATH + f\"preds_xr_{model}_{model_num}_{dataset}.zarr\"\n",
    "targ_path = PATH + f\"targets_xr_{model}_{model_num}_{dataset}.zarr\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================================================\n",
    "# Load Data\n",
    "# ============================================================================================================================\n",
    "\n",
    "# Open the existing datasets\n",
    "pred_dataset = xr.open_zarr(pred_path, consolidated=False)\n",
    "targ_dataset = xr.open_zarr(targ_path, consolidated=False)\n",
    "\n",
    "# Calculate the standard deviation for the datasets\n",
    "pred_std = pred_dataset.std(dim='time')\n",
    "targ_std = targ_dataset.std(dim='time')\n",
    "\n",
    "# Get the first sample from the dataset to extract config\n",
    "sample = pred_dataset.isel(time=0)\n",
    "pred_data = sample[\"predictions_vit\"] if model == \"vit\" else sample[\"predictions_swinlstm\"]\n",
    "\n",
    "# Access the attributes from the .zattrs file\n",
    "zattrs = pred_data.attrs\n",
    "cfg = zattrs[\"cfg\"]\n",
    "cfg[\"dataset\"] = dataset\n",
    "adjust_grid_region = cfg[\"1_2_grid\"]\n",
    "temp_ocean_levels = cfg[\"temp_ocean\"]\n",
    "cfg[\"three_month_mean\"] = three_month_mean\n",
    "cfg[\"adjust_grid_region\"] = adjust_grid_region\n",
    "cfg[\"temp_ocean_levels\"] = temp_ocean_levels\n",
    "\n",
    "# Get the time range\n",
    "total_length = len(pred_dataset['time'])\n",
    "num_batches_total = math.ceil(total_length / cfg[\"batch_size\"])\n",
    "\n",
    "# Load data and lsm mask\n",
    "if cfg[\"1_2_grid\"]:\n",
    "    val_ds_adapt = xr.open_dataset(PATH + \"/../../../../results/val_ds_adapt_1_2_grid.nc\")\n",
    "    common_lsm = xr.open_dataset(PATH + '/../../../../../../data/processed_data/enso_data_pacific/land_sea_mask_common_1_2_grid.nc')['lsm'].data\n",
    "else:\n",
    "    val_ds_adapt = xr.open_dataset(PATH + \"/../../../../results/val_ds_adapt.nc\")\n",
    "    common_lsm = xr.open_dataset(PATH + '/../../../../../../data/processed_data/enso_data_pacific/land_sea_mask_common.nc')['lsm'].data\n",
    "\n",
    "lsm = common_lsm[cfg[\"lat_range\"][0]:cfg[\"lat_range\"][1], cfg[\"lon_range\"][0]:cfg[\"lon_range\"][1]]\n",
    "\n",
    "# Initialize Loss Functions\n",
    "l2 = torch.nn.MSELoss(reduction = 'none')\n",
    "loss_fn =  losses.NormalCRPS(reduction=\"none\", mode = cfg[\"loss_mode\"], dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================================================\n",
    "# Eval Forward Pass\n",
    "# ============================================================================================================================\n",
    "\n",
    "rmse = 0\n",
    "mse = 0\n",
    "crps = 0\n",
    "nino_rmse_3 = 0\n",
    "nino_rmse_3_l = 0\n",
    "nino_rmse_34 = 0\n",
    "nino_rmse_34_l = 0\n",
    "nino_rmse_4 = 0\n",
    "nino_rmse_4_l = 0\n",
    "rmse_orig = 0\n",
    "crps_orig = 0\n",
    "\n",
    "# Initialize batch-level metrics\n",
    "batch_mse = 0\n",
    "batch_rmse = 0\n",
    "batch_rmse_orig = 0\n",
    "batch_crps = 0\n",
    "batch_crps_orig = 0\n",
    "batch_nino_rmse_3 = 0\n",
    "batch_nino_rmse_3_l = 0\n",
    "batch_nino_rmse_34 = 0\n",
    "batch_nino_rmse_34_l = 0\n",
    "batch_nino_rmse_4 = 0\n",
    "batch_nino_rmse_4_l = 0\n",
    "\n",
    "num_batches = 0\n",
    "\n",
    "predictions = []\n",
    "targets = []\n",
    "\n",
    "# Dictionary to store results for each subsample\n",
    "loss_dict = {}\n",
    "\n",
    "# Generate evenly spaced indices for subsampling\n",
    "subsample_indices = np.linspace(num_batches_total/num_subsamples, num_batches_total-1, num=num_subsamples, dtype=int) * 8\n",
    "num_subsamples = len(subsample_indices)\n",
    "print(\"Total length of dataset: \", total_length)\n",
    "print(\"Number of batches: \", num_batches_total)\n",
    "print(\"Number of subsamples: \", num_subsamples)\n",
    "print(\"Subsample indices: \", subsample_indices / 8)\n",
    "\n",
    "# Iterate over the dataset in batches of size cfg[\"batch_size\"]\n",
    "#for i in range(0, int(total_length/(num_subsamples/4)), cfg[\"batch_size\"]):\n",
    "for i in range(0, total_length, cfg[\"batch_size\"]):\n",
    "    \n",
    "    batch_end = min(i + cfg[\"batch_size\"], total_length)  # Ensure we don't exceed subsample size\n",
    "\n",
    "    # Select the corresponding batch of data for the current time steps\n",
    "    pred_slice = pred_dataset['predictions_vit'].isel(time=slice(i, batch_end)) if model == \"vit\" else pred_dataset['predictions_swinlstm'].isel(time=slice(i, batch_end))\n",
    "    target_slice = targ_dataset['targets_vit'].isel(time=slice(i, batch_end)) if model == \"vit\" else targ_dataset['targets_swinlstm'].isel(time=slice(i, batch_end))\n",
    "\n",
    "    # Convert the data slices to PyTorch tensors\n",
    "    pred = torch.tensor(pred_slice.values)  #[8, 20, 51, 120]\n",
    "    targ = torch.tensor(target_slice.values) #[8, 20, 51, 120]\n",
    "\n",
    "    # If necessary, adjust grid region\n",
    "    if adjust_grid_region:\n",
    "        pred = pred[:, :, :, 29:95] if not cfg[\"probabilistic\"] else pred[:, :, :, :, 29:95]\n",
    "        targ = targ[:, :, :, 29:95]\n",
    "        cfg[\"nino_3_lon\"] = (31, 61) \n",
    "        cfg[\"nino_34_lon\"] = (21, 46)\n",
    "        cfg[\"nino_4_lon\"] = (6, 31)\n",
    "        lsm = common_lsm[cfg[\"lat_range\"][0]:cfg[\"lat_range\"][1], 29:95]\n",
    "\n",
    "    predictions.append((pred, pred_slice.attrs[\"month\"])) if not cfg[\"probabilistic\"] else predictions.append((pred[:, 0], pred_slice.attrs[\"month\"]))\n",
    "    targets.append((targ, target_slice.attrs[\"month\"]))\n",
    "\n",
    "    # Get target shape and land-sea fraction\n",
    "    B, T, H, W = targ.shape\n",
    "    N = (B * T * ((H * W) - lsm.sum())).item()\n",
    "    N = N / T  # Adjust for time\n",
    "\n",
    "    # Probabilistic case: calculate CRPS if necessary\n",
    "    if cfg[\"probabilistic\"]:\n",
    "        crps_ = loss_fn(targ, pred)\n",
    "        crps_ *= (1 - lsm)[None, None, :, :]\n",
    "        batch_crps += crps_.sum(dim=(0, 2, 3)).div(N)\n",
    "        batch_crps_orig += crps_.mean(dim=0)\n",
    "        pred = pred[:, 0]  # Only use the mean prediction for the deterministic case\n",
    "\n",
    "    #print(\"Pred shape: \", pred.shape)\n",
    "    #print(\"Targ shape: \", targ.shape)\n",
    "\n",
    "    # Calculate MSE and RMSE\n",
    "    mse_ = l2(pred, targ)\n",
    "    mse_ *= (1 - lsm)[None, None, :, :]\n",
    "    batch_mse += mse_.sum(dim=(0, 2, 3)).div(N)\n",
    "    batch_rmse += torch.sqrt(mse_.sum(dim=(0, 2, 3)).div(N))\n",
    "    batch_rmse_orig += torch.sqrt(mse_.mean(dim=0))\n",
    "    #print(\"RMSE orig mean: \", torch.sqrt(mse_.mean(dim=0)).mean())\n",
    "    #print(\"RMSE\", torch.sqrt(mse_.sum(dim=(0, 2, 3)).div(N)))\n",
    "\n",
    "    # Calculate Nino 3, 3.4, and 4 indices\n",
    "    nino_pred_3 = pred[:, :, cfg[\"nino_3_lat\"][0]:cfg[\"nino_3_lat\"][1], cfg[\"nino_3_lon\"][0]:cfg[\"nino_3_lon\"][1]]\n",
    "    nino_true_3 = targ[:, :, cfg[\"nino_3_lat\"][0]:cfg[\"nino_3_lat\"][1], cfg[\"nino_3_lon\"][0]:cfg[\"nino_3_lon\"][1]]\n",
    "    batch_nino_rmse_3 += torch.sqrt(l2(nino_pred_3, nino_true_3).mean(dim=0))\n",
    "    batch_nino_rmse_3_l += torch.sqrt(l2(nino_pred_3, nino_true_3).mean(dim=(0, 2, 3)))\n",
    "\n",
    "\n",
    "    nino_pred_34 = pred[:, :, cfg[\"nino_3_lat\"][0]:cfg[\"nino_3_lat\"][1], cfg[\"nino_34_lon\"][0]:cfg[\"nino_34_lon\"][1]]\n",
    "    nino_true_34 = targ[:, :, cfg[\"nino_3_lat\"][0]:cfg[\"nino_3_lat\"][1], cfg[\"nino_34_lon\"][0]:cfg[\"nino_34_lon\"][1]]\n",
    "    batch_nino_rmse_34 += torch.sqrt(l2(nino_pred_34, nino_true_34).mean(dim=0))\n",
    "    batch_nino_rmse_34_l += torch.sqrt(l2(nino_pred_34, nino_true_34).mean(dim=(0, 2, 3)))\n",
    "\n",
    "\n",
    "    nino_pred_4 = pred[:, :, cfg[\"nino_3_lat\"][0]:cfg[\"nino_3_lat\"][1], cfg[\"nino_4_lon\"][0]:cfg[\"nino_4_lon\"][1]]\n",
    "    nino_true_4 = targ[:, :, cfg[\"nino_3_lat\"][0]:cfg[\"nino_3_lat\"][1], cfg[\"nino_4_lon\"][0]:cfg[\"nino_4_lon\"][1]]\n",
    "    batch_nino_rmse_4 += torch.sqrt(l2(nino_pred_4, nino_true_4).mean(dim=0))\n",
    "    batch_nino_rmse_4_l += torch.sqrt(l2(nino_pred_4, nino_true_4).mean(dim=(0, 2, 3)))\n",
    "\n",
    "\n",
    "    # Increment the batch counter\n",
    "    num_batches += 1\n",
    "\n",
    "    if i in subsample_indices:\n",
    "        # Store the batch results in the dictionary for this subsample iteration\n",
    "        loss_dict[f'subsample_{i // cfg[\"batch_size\"]}'] = {\n",
    "            'mse': batch_mse.cpu().numpy() / num_batches,\n",
    "            'rmse': batch_rmse.cpu().numpy() / num_batches,\n",
    "            'rmse_orig': batch_rmse_orig.cpu().numpy() / num_batches,\n",
    "            'crps': batch_crps.cpu().numpy() / num_batches if cfg[\"probabilistic\"] else None,\n",
    "            'crps_orig': batch_crps_orig.cpu().numpy() / num_batches if cfg[\"probabilistic\"] else None,\n",
    "            'nino_rmse_3': batch_nino_rmse_3.cpu().numpy() / num_batches,\n",
    "            'nino_rmse_3_l': batch_nino_rmse_3_l.cpu().numpy() / num_batches,\n",
    "            'nino_rmse_34': batch_nino_rmse_34.cpu().numpy() / num_batches,\n",
    "            'nino_rmse_34_l': batch_nino_rmse_34_l.cpu().numpy() / num_batches,\n",
    "            'nino_rmse_4': batch_nino_rmse_4.cpu().numpy() / num_batches,\n",
    "            'nino_rmse_4_l': batch_nino_rmse_4_l.cpu().numpy() / num_batches\n",
    "        }\n",
    "        print(\"Subsample num Batches: \", i // cfg[\"batch_size\"])\n",
    "        print(\"MSE: \", batch_mse.mean().cpu().numpy() / num_batches)\n",
    "        print(\"RMSE: \", batch_rmse.mean().cpu().numpy() / num_batches)\n",
    "        print(\"RMSE_orig: \", batch_rmse_orig.mean().cpu().numpy() / num_batches)\n",
    "        print(\"CRPS: \", batch_crps.mean().cpu().numpy() / num_batches) if cfg[\"probabilistic\"] else None\n",
    "        print(\"CRPS_orig: \", batch_crps_orig.mean().cpu().numpy() / num_batches) if cfg[\"probabilistic\"] else None\n",
    "        print(\"Nino 3 rmse: \", batch_nino_rmse_3_l.mean().cpu().numpy() / num_batches)\n",
    "        print(\"Nino 3.4 rmse: \", batch_nino_rmse_34_l.mean().cpu().numpy() / num_batches)\n",
    "        print(\"Nino 4 rmse: \", batch_nino_rmse_4_l.mean().cpu().numpy() / num_batches)\n",
    "        print(\"=====================================================================================================\")\n",
    "\n",
    "        # Accumulate the metrics across all subsamples\n",
    "        mse += batch_mse / num_batches\n",
    "        rmse += batch_rmse / num_batches\n",
    "        rmse_orig += batch_rmse_orig / num_batches\n",
    "        if cfg[\"probabilistic\"]:\n",
    "            crps += batch_crps / num_batches\n",
    "            crps_orig += batch_crps_orig / num_batches\n",
    "        nino_rmse_3 += batch_nino_rmse_3 / num_batches\n",
    "        nino_rmse_3_l += batch_nino_rmse_3_l / num_batches\n",
    "        nino_rmse_34 += batch_nino_rmse_34 / num_batches\n",
    "        nino_rmse_34_l += batch_nino_rmse_34_l / num_batches\n",
    "        nino_rmse_4 += batch_nino_rmse_4 / num_batches\n",
    "        nino_rmse_4_l += batch_nino_rmse_4_l / num_batches\n",
    "\n",
    "        # Initialize batch-level metrics\n",
    "        batch_mse = 0\n",
    "        batch_rmse = 0\n",
    "        batch_rmse_orig = 0\n",
    "        batch_crps = 0\n",
    "        batch_crps_orig = 0\n",
    "        batch_nino_rmse_3 = 0\n",
    "        batch_nino_rmse_3_l = 0\n",
    "        batch_nino_rmse_34 = 0\n",
    "        batch_nino_rmse_34_l = 0\n",
    "        batch_nino_rmse_4 = 0\n",
    "        batch_nino_rmse_4_l = 0\n",
    "\n",
    "        num_batches = 0\n",
    "\n",
    "\n",
    "\n",
    "# After all batches/subsamples, store the summed-up results in the dictionary\n",
    "loss_dict['total'] = {\n",
    "    'mse': mse.cpu().numpy() / num_subsamples,\n",
    "    'rmse': rmse.cpu().numpy() / num_subsamples,\n",
    "    'rmse_orig': rmse_orig.cpu().numpy() / num_subsamples,\n",
    "    'crps': crps.cpu().numpy() / num_subsamples if cfg[\"probabilistic\"] else None,\n",
    "    'crps_orig': crps_orig.cpu().numpy() / num_subsamples if cfg[\"probabilistic\"] else None,\n",
    "    'nino_rmse_3': nino_rmse_3.cpu().numpy() / num_subsamples,\n",
    "    'nino_rmse_3_l': nino_rmse_3_l.cpu().numpy() / num_subsamples,\n",
    "    'nino_rmse_34': nino_rmse_34.cpu().numpy() / num_subsamples,\n",
    "    'nino_rmse_34_l': nino_rmse_34_l.cpu().numpy() / num_subsamples,\n",
    "    'nino_rmse_4': nino_rmse_4.cpu().numpy() / num_subsamples,\n",
    "    'nino_rmse_4_l': nino_rmse_4_l.cpu().numpy() / num_subsamples,\n",
    "    'pred_data_std': pred_std,\n",
    "    'targ_data_std': targ_std\n",
    "}\n",
    "\n",
    "rmse /= num_subsamples\n",
    "mse /= num_subsamples\n",
    "crps /= num_subsamples\n",
    "nino_rmse_3 /= num_subsamples\n",
    "nino_rmse_3_l /= num_subsamples\n",
    "nino_rmse_34 /= num_subsamples\n",
    "nino_rmse_34_l /= num_subsamples\n",
    "nino_rmse_4 /= num_subsamples\n",
    "nino_rmse_4_l /= num_subsamples\n",
    "rmse_orig /= num_subsamples\n",
    "crps_orig /= num_subsamples\n",
    "\n",
    "print(\"MSE over validation set: \", mse.mean(), mse.shape)\n",
    "print(\"RMSE-div(N)-mean over validation set: \", rmse.mean(), rmse.shape)\n",
    "print(\"RMSE-original over validation set: \", rmse_orig.mean(), rmse_orig.shape)\n",
    "print(\"CRPS over validation set: \", crps.mean(), crps.shape) if cfg[\"probabilistic\"] else None\n",
    "print(\"CRPS_orig over validation set: \", crps_orig.mean(), crps_orig.shape) if cfg[\"probabilistic\"] else None\n",
    "print(\"Nino 3 rmse: \", nino_rmse_3_l.mean(), nino_rmse_3.mean(), nino_rmse_3.shape)\n",
    "print(\"Nino 3.4 rmse: \", nino_rmse_34_l.mean(), nino_rmse_34.mean(), nino_rmse_34.shape)\n",
    "print(\"Nino 4 rmse: \", nino_rmse_4_l.mean(), nino_rmse_4.mean(), nino_rmse_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================================================\n",
    "# Calculate Climatology\n",
    "# ============================================================================================================================\n",
    "\n",
    "if dataset == \"oras5\":\n",
    "    val_range = slice('1983-01-16', \"2022-09-16\")\n",
    "elif dataset == \"cesm2_picontrol\":\n",
    "    val_range = slice('1700-01-01', \"2022-09-16\")\n",
    "elif dataset == \"cesm2_lens\":\n",
    "    val_range = slice('1850-01-01', \"2022-09-16\")\n",
    "else:\n",
    "    val_range = slice(None, None)\n",
    "\n",
    "# Check if ocean temperature is given in levels\n",
    "if temp_ocean_levels:\n",
    "\n",
    "    if cfg[\"1_2_grid\"]:\n",
    "        if dataset == \"cesm2_picontrol\":\n",
    "            ds_sst_level0 = xr.open_dataset(\"C:/Users/felix/PycharmProjects/deeps2a-enso/data/test_data/cesm2_picontrol_temp_ocean_1_2_grid_combined_level0.nc\")\n",
    "        elif dataset == \"oras5\":\n",
    "            ds_sst_level0 = xr.open_dataset(\"C:/Users/felix/PycharmProjects/deeps2a-enso/data/test_data/temp_ocn_0a_lat-31_33_lon90_330_gr1.0_2.0_level0.nc\")\n",
    "        else:\n",
    "            ds_sst_level0 = xr.open_dataset(\"C:/Users/felix/PycharmProjects/deeps2a-enso/data/test_data/ensemble_1301_001_temp_ocn_0a_lat-31_33_lon90_330_gr1.0_2.0_1850-2015.nc\")\n",
    "    else:\n",
    "        if dataset == \"cesm2_picontrol\":\n",
    "            ds_sst_level0 = xr.open_dataset(\"C:/Users/felix/PycharmProjects/deeps2a-enso/data/test_data/cesm2_picontrol_temp_ocean_combined_level0.nc\")\n",
    "        elif dataset == \"oras5\":\n",
    "            ds_sst_level0 = xr.open_dataset(\"C:/Users/felix/PycharmProjects/deeps2a-enso/data/test_data/temp_ocn_0a_lat-31_33_lon130_290_gr1.0_1.0_level0.nc\")\n",
    "        else:\n",
    "            ds_sst_level0 = xr.open_dataset(\"C:/Users/felix/PycharmProjects/deeps2a-enso/data/test_data/ensemble_1301_001_temp_ocn_0a_lat-31_33_lon130_290_gr1.0_1.0_1850-2015.nc\")\n",
    "\n",
    "    # Calculate climatology -> data already in anomalies\n",
    "    ds_sst_level0 = ds_sst_level0.sel(time=val_range)\n",
    "    ds_sst_level0_nino3 = ds_sst_level0.sel(lat=slice(-4, 5), lon=slice(210, 269))\n",
    "    ds_sst_level0_nino34 = ds_sst_level0.sel(lat=slice(-4, 5), lon=slice(190, 239))\n",
    "    ds_sst_level0_nino4 = ds_sst_level0.sel(lat=slice(-4, 5), lon=slice(174, 223))\n",
    "\n",
    "    rmse_sst_level0_spatial = np.sqrt(((ds_sst_level0['temp_ocn_0a']**2).mean('time')))\n",
    "    rmse_sst_level0 = np.sqrt((ds_sst_level0['temp_ocn_0a']**2).mean(dim=['time', 'lat', 'lon']))\n",
    "    rmse_sst_level0_nino3 = np.sqrt((ds_sst_level0_nino3['temp_ocn_0a']**2).mean(dim=['time', 'lat', 'lon']))\n",
    "    rmse_sst_level0_nino34 = np.sqrt((ds_sst_level0_nino34['temp_ocn_0a']**2).mean(dim=['time', 'lat', 'lon']))\n",
    "    rmse_sst_level0_nino4 = np.sqrt((ds_sst_level0_nino4['temp_ocn_0a']**2).mean(dim=['time', 'lat', 'lon']))\n",
    "\n",
    "    print(\"RMSE Climatology Mean\",rmse_sst_level0.mean())\n",
    "    print(\"RMSE Climatology Spatial\", rmse_sst_level0_spatial.mean())\n",
    "\n",
    "    # if not anomalies -> rmse_ssh = np.sqrt(((ds_oras5_ssh.groupby('time.month') - da_oras5_ssh_climatology)**2).mean('time'))\n",
    "\n",
    "else:\n",
    "    if cfg[\"dataset\"] == \"cesm2_picontrol\":\n",
    "        ds_sst = xr.open_dataset(\"C:/Users/felix/PycharmProjects/deeps2a-enso/data/test_data/b.e21.B1850.f09_g17.CMIP6-piControl.001.pop.h.ssta_lat-31_33_lon130_290_gr1.0.nc\")\n",
    "    elif cfg[\"dataset\"] == \"oras5\":\n",
    "        ds_sst = xr.open_dataset(\"C:/Users/felix/PycharmProjects/deeps2a-enso/data/test_data/ssta_lat-31_33_lon130_290_gr1.0.nc\")\n",
    "    else:\n",
    "        ds_sst = xr.open_dataset(\"C:/Users/felix/PycharmProjects/deeps2a-enso/data/test_data/b.e21.BHISTcmip6.f09_g17.LE2-1301.001.ssta_lat-31_33_lon130_290_gr1.0.nc\")\n",
    "\n",
    "    # Calculate climatology -> data already in anomalies\n",
    "    ds_sst = ds_sst.sel(time=val_range)\n",
    "    rmse_sst_spatial = np.sqrt(((ds_sst['ssta']**2).mean('time')))\n",
    "    rmse_sst = np.sqrt(((ds_sst['ssta']**2).mean(dim=['time', 'lat', 'lon'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "def extract_images_from_pdfs(pdf_paths):\n",
    "    images = []\n",
    "    for pdf_path in pdf_paths:\n",
    "        pdf_images = convert_from_path(pdf_path)\n",
    "        images.extend(pdf_images)\n",
    "    return images\n",
    "\n",
    "def create_gif_from_images(images, output_gif_path, duration=500, loop=0):\n",
    "    if images:\n",
    "        images[0].save(\n",
    "            output_gif_path,\n",
    "            save_all=True,\n",
    "            append_images=images[1:],\n",
    "            duration=duration,\n",
    "            loop=loop\n",
    "        )\n",
    "        print(f\"GIF saved to {output_gif_path}\")\n",
    "    else:\n",
    "        print(\"No images to create a GIF.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================================================\n",
    "# Generate Hindcast Plots with Sufficient Space for Colorbar\n",
    "# ============================================================================================================================\n",
    "\n",
    "\n",
    "lags = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
    "val_ds_adapt_ = val_ds_adapt.sel(lat=slice(-25, 25))\n",
    "\n",
    "if dataset == \"cesm2_picontrol\":\n",
    "    choose_samples = [10, 25, 50]  # Total samples 297\n",
    "else:\n",
    "    choose_samples = [7, 9, 10, 14, 15, 18, 20, 21, 25, 27, 30, 35]  # Total samples 53\n",
    "\n",
    "for sample in choose_samples:\n",
    "    create_directory(save_dir_ + f\"{\"sample_\" + str(sample)}\")\n",
    "\n",
    "for sample in choose_samples:\n",
    "\n",
    "    pred, _ = predictions[sample]\n",
    "    targ, _ = targets[sample]\n",
    "\n",
    "    if model == \"swinlstm\":\n",
    "        pred = pred[:, :, 0:51, :]\n",
    "        targ = targ[:, :, 0:51, :]\n",
    "        lsm = lsm[0:51, :]\n",
    "        model_name = \"SwinLSTM\"\n",
    "    else:\n",
    "        model_name = \"3D-Geoformer\"\n",
    "\n",
    "    if adjust_grid_region:\n",
    "        val_ds_adapt_ = val_ds_adapt.sel(lat=slice(-26, 25), lon=slice(148, 279))\n",
    "\n",
    "    for lag in lags:\n",
    "\n",
    "        hindcast = dict()\n",
    "        for key, x in dict(Prediction=pred.unsqueeze(1), Target=targ.unsqueeze(1)).items():\n",
    "\n",
    "            x_lst = []\n",
    "            x = x.cpu()\n",
    "            for i, var in enumerate(val_ds_adapt_.data_vars):\n",
    "                da = xr.DataArray(\n",
    "                    data=x[:, i, lag - 1],\n",
    "                    coords=val_ds_adapt_.isel(time=[1, 2, 3, 4, 5, 6, 7, 8]).coords,\n",
    "                    name=var)\n",
    "\n",
    "                # Mask land\n",
    "                da = da.where(lsm == 0, other=np.nan)\n",
    "\n",
    "                # Add to list\n",
    "                x_lst.append(da)\n",
    "                break\n",
    "            hindcast[key] = xr.merge(x_lst)\n",
    "\n",
    "        # Create a figure with 2 subplots sharing the same colorbar and projection\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(14, 7), subplot_kw={'projection': ccrs.PlateCarree(central_longitude=180), 'aspect': 'auto'})\n",
    "\n",
    "        # Define a list of keys, titles, and colorbar parameters\n",
    "        keys = ['Prediction', 'Target']\n",
    "        titles = ['Prediction', 'Target']\n",
    "        cmap = 'RdBu_r'\n",
    "        vmin = -2.5\n",
    "        vmax = 2.5\n",
    "\n",
    "        # Flatten the axes array for easier iteration\n",
    "        axs = axs.ravel()\n",
    "\n",
    "        for ax, key, title in zip(axs, keys, titles):\n",
    "            x = hindcast[key]\n",
    "            for var in x.data_vars:\n",
    "                im = ax.pcolormesh(x.lon, x.lat, x[var].isel(time=0), cmap=cmap, vmin=vmin, vmax=vmax, transform=ccrs.PlateCarree())\n",
    "                ax.coastlines()\n",
    "                ax.set_title(title, fontsize=20, pad=15, fontweight=\"bold\")  # Adjusted padding for subplot titles\n",
    "                ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "                ax.add_feature(cfeature.LAND, facecolor='lightgray')\n",
    "                gl = ax.gridlines(draw_labels=True, crs=ccrs.PlateCarree(), linewidth=0.3, color='gray', alpha=0.5, linestyle='--')\n",
    "                gl.top_labels = False  # Remove x-labels from the top\n",
    "                gl.right_labels = False  # Remove y-labels from the right\n",
    "                gl.xlabel_style = {'fontsize': 18}  # Increase x-label font size\n",
    "                gl.ylabel_style = {'fontsize': 18}  # Increase y-label font size\n",
    "                if ax == axs[1]:  # For the second subplot (right), remove y-labels\n",
    "                    gl.left_labels = False  # Remove y-labels from the left side of the second plot\n",
    "\n",
    "                # Add contour lines\n",
    "                ax.contour(x.lon, x.lat, x[var].isel(time=0), levels=2, colors='k', linewidths=0.3, transform=ccrs.PlateCarree())\n",
    "\n",
    "        # Add a shared colorbar below the plots with more space\n",
    "        #cbar_ax = fig.add_axes([0.2, 0.1, 0.65, 0.03])  # Adjusted y-position and height of colorbar\n",
    "        #cbar = fig.colorbar(im, cax=cbar_ax, orientation='horizontal', extend='both')\n",
    "        #cbar.set_label('SSTA [°C]', fontsize=21)\n",
    "        #cbar.ax.tick_params(labelsize=19)\n",
    "\n",
    "        # Add a main title for the entire figure\n",
    "        fig.suptitle(f'{model_name} SSTA Forecast at Month {lag}', fontsize=25, fontweight='bold', y=0.96)  # Adjusted to bring suptitle closer to plots\n",
    "\n",
    "        # Improve layout, reduce space around plots and add padding for the colorbar\n",
    "        plt.tight_layout(rect=[0, 0.2, 1, 0.95])  # Increased bottom padding to make space for the colorbar\n",
    "        fig.subplots_adjust(bottom=0.2)  # Ensure there’s enough space for the colorbar\n",
    "        #plt.show()\n",
    "        fig.savefig(f'{save_dir_}/{\"sample_\" + str(sample)}/hindcast_comparison_{model}_lag_{lag}.pdf', bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    # Paths to your PDF files\n",
    "    pdf_paths = [f'{save_dir_}/sample_{sample}/hindcast_comparison_{model}_lag_{lag}.pdf' for lag in lags]  # Replace with your PDF file paths\n",
    "\n",
    "    # Extract images from PDFs\n",
    "    images = extract_images_from_pdfs(pdf_paths)\n",
    "\n",
    "    # Path for the output GIF\n",
    "    output_gif_path = f\"{save_dir_}/{\"sample_\" + str(sample)}/lag_{lag}_output_animation_{model}.gif\"\n",
    "\n",
    "    # Create a GIF from the images\n",
    "    create_gif_from_images(images, output_gif_path, duration=500, loop=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================================================\n",
    "# Calculate Skillscore\n",
    "# ============================================================================================================================\n",
    "'''\n",
    "\n",
    "# New dictionary to store skill scores for each subsample and total results\n",
    "skillscore_dict = {}\n",
    "\n",
    "# Initialize variables to accumulate the total skill scores across all subsamples\n",
    "total_skillscore_mean = []\n",
    "total_skillscore_mean_3i = []\n",
    "total_skillscore_mean_34i = []\n",
    "total_skillscore_mean_4i = []\n",
    "\n",
    "\n",
    "# Iterate over each subsample stored in the results_dict\n",
    "for subsample_key, subsample_results in loss_dict.items():\n",
    "\n",
    "    if subsample_key == 'total':\n",
    "        continue\n",
    "\n",
    "    # Initialize lists to store skill scores for the current subsample\n",
    "    skillscore_mean = []\n",
    "    skillscore_ = []\n",
    "    skillscore_mean_3i = []\n",
    "    skillscore_3i = []\n",
    "    skillscore_mean_34i = []\n",
    "    skillscore_34i = []\n",
    "    skillscore_mean_4i = []\n",
    "    skillscore_4i = []\n",
    "\n",
    "    # Calculate skillscore for the entire region\n",
    "    loss_spatial = subsample_results['rmse_orig'][:, 0:51, :]  # Use the RMSE values for this subsample\n",
    "    loss_climatology_spatial = rmse_sst_level0_spatial.sel(lat=slice(-25, 25), lon=slice(148, 279))  # Climatology data for skill score calculation\n",
    "    loss_climatology = rmse_sst_level0.item()\n",
    "\n",
    "    for lag in range(20):\n",
    "        rmse_lag_spatial = xr.DataArray(loss_spatial[lag], dims=['lat', 'lon'], coords={'lat': val_ds_adapt_.lat, 'lon': val_ds_adapt_.lon})\n",
    "        rmse_lag = loss_spatial[lag].mean()\n",
    "        skillscore = 1 - (rmse_lag / loss_climatology)\n",
    "        skillscore_spatial = 1 - (rmse_lag_spatial / loss_climatology_spatial)\n",
    "        skillscore_.append(skillscore_spatial)\n",
    "        skillscore_mean.append(skillscore.item())\n",
    "\n",
    "    # Calculate skillscore for the Nino 3 region\n",
    "    loss_spatial = subsample_results['nino_rmse_3'][:, 0:51, :]\n",
    "    val_ds_3i = val_ds_adapt.sel(lat=slice(-4, 5), lon=slice(210, 269))\n",
    "    loss_climatology_3i_spatial = rmse_sst_level0_spatial.sel(lat=slice(-4, 5), lon=slice(210, 269))\n",
    "    loss_climatology_3i = rmse_sst_level0_nino3.item()\n",
    "\n",
    "    for lag in range(20):\n",
    "        rmse_lag_spatial = xr.DataArray(loss_spatial[lag], dims=['lat', 'lon'], coords={'lat': val_ds_3i.lat, 'lon': val_ds_3i.lon})\n",
    "        rmse_lag = loss_spatial[lag].mean()\n",
    "        skillscore = 1 - (rmse_lag / loss_climatology_3i)\n",
    "        skillscore_spatial = 1 - (rmse_lag_spatial / loss_climatology_3i_spatial)\n",
    "        skillscore_3i.append(skillscore_spatial)\n",
    "        skillscore_mean_3i.append(skillscore.item())\n",
    "\n",
    "    # Calculate skillscore for the Nino 3.4 region\n",
    "    loss_spatial = subsample_results['nino_rmse_34'][:, 0:51, :]\n",
    "    val_ds_34i = val_ds_adapt.sel(lat=slice(-4, 5), lon=slice(190, 239))\n",
    "    loss_climatology_34i_spatial = rmse_sst_level0_spatial.sel(lat=slice(-4, 5), lon=slice(190, 239))\n",
    "    loss_climatology_34i = rmse_sst_level0_nino34.item()\n",
    "\n",
    "    for lag in range(20):\n",
    "        rmse_lag_spatial = xr.DataArray(loss_spatial[lag], dims=['lat', 'lon'], coords={'lat': val_ds_34i.lat, 'lon': val_ds_34i.lon})\n",
    "        rmse_lag = loss_spatial[lag].mean()\n",
    "        skillscore = 1 - (rmse_lag / loss_climatology_34i)\n",
    "        skillscore_spatial = 1 - (rmse_lag_spatial / loss_climatology_34i_spatial)\n",
    "        skillscore_34i.append(skillscore_spatial)\n",
    "        skillscore_mean_34i.append(skillscore.item())\n",
    "\n",
    "    # Calculate skillscore for the Nino 4 region\n",
    "    loss_spatial = subsample_results['nino_rmse_4'][:, 0:51, :]\n",
    "    val_ds_4i = val_ds_adapt.sel(lat=slice(-4, 5), lon=slice(174, 223))\n",
    "    loss_climatology_4i_spatial = rmse_sst_level0_spatial.sel(lat=slice(-4, 5), lon=slice(174, 223))\n",
    "    loss_climatology_4i = rmse_sst_level0_nino4.item()\n",
    "\n",
    "    for lag in range(20):\n",
    "        rmse_lag_spatial = xr.DataArray(loss_spatial[lag], dims=['lat', 'lon'], coords={'lat': val_ds_4i.lat, 'lon': val_ds_4i.lon})\n",
    "        rmse_lag = loss_spatial[lag].mean()\n",
    "        skillscore = 1 - (rmse_lag / loss_climatology_4i)\n",
    "        skillscore_spatial = 1 - (rmse_lag_spatial / loss_climatology_4i_spatial)\n",
    "        skillscore_4i.append(skillscore_spatial)\n",
    "        skillscore_mean_4i.append(skillscore.item())\n",
    "\n",
    "    print(f\"Skillscore for subsample {subsample_key}:\")\n",
    "    print(\"Mean Skillscore: \", skillscore_mean)\n",
    "    print(\"Mean Skillscore 3i: \", skillscore_mean_3i)\n",
    "    print(\"Mean Skillscore 34i: \", skillscore_mean_34i)\n",
    "    print(\"Mean Skillscore 4i: \", skillscore_mean_4i)\n",
    "    print(\"=====================================================================================================\")\n",
    "\n",
    "    # Store the skill scores for this subsample in the skillscore_dict\n",
    "    skillscore_dict[subsample_key] = {\n",
    "        'skillscore_mean': skillscore_mean,\n",
    "        'skillscore_': skillscore_,\n",
    "        'skillscore_mean_3i': skillscore_mean_3i,\n",
    "        'skillscore_3i': skillscore_3i,\n",
    "        'skillscore_mean_34i': skillscore_mean_34i,\n",
    "        'skillscore_34i': skillscore_34i,\n",
    "        'skillscore_mean_4i': skillscore_mean_4i,\n",
    "        'skillscore_4i': skillscore_4i\n",
    "    }\n",
    "\n",
    "    if not total_skillscore_mean:\n",
    "        total_skillscore_mean = skillscore_mean\n",
    "        total_skillscore_ = skillscore_\n",
    "        total_skillscore_mean_3i = skillscore_mean_3i\n",
    "        total_skillscore_3i = skillscore_3i\n",
    "        total_skillscore_mean_34i = skillscore_mean_34i\n",
    "        total_skillscore_34i = skillscore_34i\n",
    "        total_skillscore_mean_4i = skillscore_mean_4i\n",
    "        total_skillscore_4i = skillscore_4i\n",
    "    else:\n",
    "        # Accumulate skill scores for calculating total results\n",
    "        total_skillscore_mean = [a + b for a, b in zip(total_skillscore_mean, skillscore_mean)]\n",
    "        total_skillscore_ = [a + b for a, b in zip(total_skillscore_, skillscore_)]\n",
    "        total_skillscore_mean_3i = [a + b for a, b in zip(total_skillscore_mean_3i, skillscore_mean_3i)]\n",
    "        total_skillscore_3i = [a + b for a, b in zip(total_skillscore_3i, skillscore_3i)]\n",
    "        total_skillscore_mean_34i = [a + b for a, b in zip(total_skillscore_mean_34i, skillscore_mean_34i)]\n",
    "        total_skillscore_34i = [a + b for a, b in zip(total_skillscore_34i, skillscore_34i)]\n",
    "        total_skillscore_mean_4i = [a + b for a, b in zip(total_skillscore_mean_4i, skillscore_mean_4i)]\n",
    "        total_skillscore_4i = [a + b for a, b in zip(total_skillscore_4i, skillscore_4i)]\n",
    "\n",
    "total_skillscore_mean = [a / num_subsamples for a in total_skillscore_mean]\n",
    "total_skillscore_ = [a / num_subsamples for a in total_skillscore_]\n",
    "total_skillscore_mean_3i = [a / num_subsamples for a in total_skillscore_mean_3i]\n",
    "total_skillscore_3i = [a / num_subsamples for a in total_skillscore_3i]\n",
    "total_skillscore_mean_34i = [a / num_subsamples for a in total_skillscore_mean_34i]\n",
    "total_skillscore_34i = [a / num_subsamples for a in total_skillscore_34i]\n",
    "total_skillscore_mean_4i = [a / num_subsamples for a in total_skillscore_mean_4i]\n",
    "total_skillscore_4i = [a / num_subsamples for a in total_skillscore_4i]\n",
    "\n",
    "# After all subsamples, calculate the total skill scores across all subsamples\n",
    "skillscore_dict['total'] = {\n",
    "    'skillscore_mean': total_skillscore_mean,\n",
    "    'skillscore_': total_skillscore_,\n",
    "    'skillscore_mean_3i': total_skillscore_mean_3i,\n",
    "    'skillscore_3i': total_skillscore_3i,\n",
    "    'skillscore_mean_34i': total_skillscore_mean_34i,\n",
    "    'skillscore_34i': total_skillscore_34i,\n",
    "    'skillscore_mean_4i': total_skillscore_mean_4i,\n",
    "    'skillscore_4i': total_skillscore_4i\n",
    "}\n",
    "\n",
    "# Print the final total skill scores\n",
    "print(\"Final Total Skill Scores:\")\n",
    "for metric_, value in skillscore_dict['total'].items():\n",
    "    if 'mean' in metric_:\n",
    "        print(f\"{metric_}: {value}\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================================================\n",
    "# Calculate Monthly Anomaly Correlation Coefficient\n",
    "# ============================================================================================================================\n",
    "'''\n",
    "pred_dataset_ = pred_dataset[\"predictions_vit\"] if model == \"vit\" else pred_dataset[\"predictions_swinlstm\"]\n",
    "targ_dataset_ = targ_dataset[\"targets_vit\"] if model == \"vit\" else targ_dataset[\"targets_swinlstm\"]\n",
    "\n",
    "if dataset == \"cesm2_picontrol\":\n",
    "    if len(pred_dataset['time']) == 15576:\n",
    "        # Slice the datasets to the possible time range 1700-2000 if whole dataset is given\n",
    "        pred_dataset_ = pred_dataset_.isel(time=slice(12000, 15576))\n",
    "        targ_dataset_ = targ_dataset_.isel(time=slice(12000, 15576))\n",
    "    start_year = 1700\n",
    "elif dataset == \"oras5\":\n",
    "    start_year = 1983\n",
    "else:\n",
    "    start_year = 1850\n",
    "\n",
    "n_time_steps = pred_dataset_.time.size\n",
    "\n",
    "start_date = datetime.datetime(start_year, 1, 1)\n",
    "date_range_targ = [start_date + relativedelta(months=i) for i in range(n_time_steps)]\n",
    "date_range_pred = [start_date + relativedelta(months=i) for i in range(n_time_steps)]\n",
    "\n",
    "# Assign the generated date range to the 'time' coordinate of the DataArray\n",
    "targ_dataset_ = targ_dataset_.assign_coords(time=date_range_targ)\n",
    "pred_dataset_ = pred_dataset_.assign_coords(time=date_range_pred)\n",
    "\n",
    "pred_months = pred_dataset_.groupby('time.month')\n",
    "targ_months = targ_dataset_.groupby('time.month')\n",
    "\n",
    "# Initialize a dictionary to store pred-targ pairs for each month\n",
    "monthly_data = {}\n",
    "\n",
    "# Iterate over both groups simultaneously using zip\n",
    "for (pred_month, pred_group), (targ_month, targ_group) in zip(pred_months, targ_months):\n",
    "    # Check if the months are aligned\n",
    "    assert pred_month == targ_month, f\"Mismatch in months: {pred_month} != {targ_month}\"\n",
    "\n",
    "    # Convert the xarray.DataArray to NumPy arrays and then to PyTorch tensors\n",
    "    pred_tensor = torch.tensor(pred_group.values)\n",
    "    targ_tensor = torch.tensor(targ_group.values)\n",
    "\n",
    "    pred_tensor = pred_tensor[:, 0,] if cfg[\"probabilistic\"] else pred_tensor\n",
    "\n",
    "    # Adjust the grid region if necessary\n",
    "    if adjust_grid_region:\n",
    "        pred_tensor = pred_tensor[:, :, :, 29:95] if not cfg[\"probabilistic\"] else pred[:, :, :, :, 29:95]\n",
    "        targ_tensor = targ_tensor[:, :, :, 29:95]\n",
    "    \n",
    "    # Save the pred and targ data in the dictionary under the respective month\n",
    "    monthly_data[pred_month] = {\n",
    "        'pred': [(pred_tensor, None)],\n",
    "        'targ': [(targ_tensor, None)]\n",
    "    }\n",
    "\n",
    "\n",
    "monthly_acc = {}\n",
    "\n",
    "for month in monthly_data.keys():\n",
    "\n",
    "    predictions = monthly_data[month]['pred']\n",
    "    targets = monthly_data[month]['targ']\n",
    "\n",
    "    #print(\"predictions\", predictions[0].shape)\n",
    "\n",
    "    results_month = {}\n",
    "\n",
    "    # Initialize lists to store ACC values for this subsample\n",
    "    acc_mean = []\n",
    "    acc_mean_spatial = []\n",
    "    acc_mean_3i = []\n",
    "    acc_mean_3i_spatial = []\n",
    "    acc_mean_34i = []\n",
    "    acc_mean_34i_spatial = []\n",
    "    acc_mean_4i = []\n",
    "    acc_mean_4i_spatial = []\n",
    "\n",
    "    # Iterate over lags (assuming 20 lags)\n",
    "    for lag in range(20):\n",
    "        # Calculate ACC for the entire region\n",
    "        acc = metric.anomaly_correlation_coefficient(predictions, targets, lag, cfg)\n",
    "        acc_mean.append(np.mean(acc))\n",
    "        \n",
    "        acc_spatial = metric.anomaly_correlation_coefficient(predictions, targets, lag, cfg, keep_spatial_coords=True)\n",
    "        stacked_np = np.stack(acc_spatial)\n",
    "        mean_acc_spatial = np.mean(stacked_np, axis=0)\n",
    "        acc_mean_spatial.append(mean_acc_spatial)\n",
    "\n",
    "        # Calculate ACC for Nino 3 region\n",
    "        acc_3i = metric.anomaly_correlation_coefficient(predictions, targets, lag, cfg, nino_index=\"nino_3\")\n",
    "        acc_mean_3i.append(np.mean(acc_3i))\n",
    "\n",
    "        acc_3i_spatial = metric.anomaly_correlation_coefficient(predictions, targets, lag, cfg, nino_index=\"nino_3\", keep_spatial_coords=True)\n",
    "        stacked_3i_np = np.stack(acc_3i_spatial)\n",
    "        mean_acc_3i_spatial = np.mean(stacked_3i_np, axis=0)\n",
    "        acc_mean_3i_spatial.append(mean_acc_3i_spatial)\n",
    "\n",
    "        # Calculate ACC for Nino 3.4 region\n",
    "        acc_34i = metric.anomaly_correlation_coefficient(predictions, targets, lag, cfg, nino_index=\"nino_34\")\n",
    "        acc_mean_34i.append(np.mean(acc_34i))\n",
    "\n",
    "        acc_34i_spatial = metric.anomaly_correlation_coefficient(predictions, targets, lag, cfg, nino_index=\"nino_34\", keep_spatial_coords=True)\n",
    "        stacked_34i_np = np.stack(acc_34i_spatial)\n",
    "        mean_acc_34i_spatial = np.mean(stacked_34i_np, axis=0)\n",
    "        acc_mean_34i_spatial.append(mean_acc_34i_spatial)\n",
    "\n",
    "        # Calculate ACC for Nino 4 region\n",
    "        acc_4i = metric.anomaly_correlation_coefficient(predictions, targets, lag, cfg, nino_index=\"nino_4\")\n",
    "        acc_mean_4i.append(np.mean(acc_4i))\n",
    "\n",
    "        acc_4i_spatial = metric.anomaly_correlation_coefficient(predictions, targets, lag, cfg, nino_index=\"nino_4\", keep_spatial_coords=True)\n",
    "        stacked_4i_np = np.stack(acc_4i_spatial)\n",
    "        mean_acc_4i_spatial = np.mean(stacked_4i_np, axis=0)\n",
    "        acc_mean_4i_spatial.append(mean_acc_4i_spatial)\n",
    "\n",
    "    print(f\"ACC for month {month}:\")\n",
    "    print(\"Mean ACC: \", acc_mean)\n",
    "    print(\"Mean ACC Nino 3: \", acc_mean_3i)\n",
    "    print(\"Mean ACC Nino 3.4: \", acc_mean_34i)\n",
    "    print(\"Mean ACC Nino 4: \", acc_mean_4i)\n",
    "\n",
    "    results_month[\"acc_mean\"] = acc_mean\n",
    "    results_month[\"acc_mean_3i\"] = acc_mean_3i\n",
    "    results_month[\"acc_mean_34i\"] = acc_mean_34i\n",
    "    results_month[\"acc_mean_4i\"] = acc_mean_4i\n",
    "    results_month[\"acc_mean_spatial\"] = acc_mean_spatial\n",
    "    results_month[\"acc_mean_3i_spatial\"] = acc_mean_3i_spatial\n",
    "    results_month[\"acc_mean_34i_spatial\"] = acc_mean_34i_spatial\n",
    "    results_month[\"acc_mean_4i_spatial\"] = acc_mean_4i_spatial\n",
    "\n",
    "    monthly_acc[month] = results_month'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================================================\n",
    "# Calculate 3-month rolling mean ACC\n",
    "# ============================================================================================================================\n",
    "'''\n",
    "if three_month_mean:\n",
    "    print(\"Applying 3-month rolling mean to target values...\")\n",
    "\n",
    "    window_size = 3  # 3-month rolling window\n",
    "\n",
    "    # Apply rolling window mean over the 'time' dimension\n",
    "    target_slice = targ_dataset_.isel(lags=0)\n",
    "\n",
    "    # Select the last time step and take all entries of the 'lags' dimension\n",
    "    last_time_step_lags = targ_dataset_.isel(time=-1).isel(lags=slice(0, None))\n",
    "\n",
    "    # Get the last value of the 'time' dimension from the target_slice\n",
    "    last_time_value = target_slice.time.values[-1].astype('datetime64[D]')\n",
    "\n",
    "    # Create new 'time' values for the new DataArray from last_time_step_lags\n",
    "    new_time_values = [last_time_value + np.timedelta64(i, 'D') for i in range(1, last_time_step_lags.lags.size + 1)]\n",
    "\n",
    "    # Create a new DataArray from last_time_step_lags, with 'time' coordinates extended\n",
    "    new_dataarray = xr.DataArray(\n",
    "        data=last_time_step_lags.values,  # Use the values from last_time_step_lags\n",
    "        dims=target_slice.dims,  # Same dimensions as target_slice\n",
    "        coords={  # Coordinate system: 'time', 'lat', and 'lon'\n",
    "            'time': new_time_values,\n",
    "            'lat': target_slice.lat,\n",
    "            'lon': target_slice.lon\n",
    "        }\n",
    "    )\n",
    "    # Concatenate the new DataArray with the original target_slice along the 'time' dimension\n",
    "    target_slice = xr.concat([target_slice, new_dataarray], dim='time')\n",
    "\n",
    "    # Compute the rolling mean with a window of 3 along the 'time' dimension\n",
    "    # 'center=True' ensures the window is centered at the current time step\n",
    "    target_slice_rolling = target_slice.rolling(time=window_size, center=True).mean()\n",
    "\n",
    "    # Now handle the edge cases (first and last entries)\n",
    "    # The first entry should remain the same as the original value since there's no previous data\n",
    "    target_slice_rolling = target_slice_rolling.copy()  # Make a copy to safely modify the data\n",
    "    target_slice_rolling.isel(time=0).data = target_slice.isel(time=0).data  # Use original value for first entry\n",
    "\n",
    "    # The last entry should also remain the same as the original value since there's no subsequent data\n",
    "    target_slice_rolling.isel(time=-1).data = target_slice.isel(time=-1).data  # Use original value for last entry\n",
    "\n",
    "    print(\"3-month rolling mean targets have been created...\")\n",
    "\n",
    "# Initialize lists to store predictions and targets at every iteration\n",
    "predictions = []\n",
    "targets = []\n",
    "\n",
    "# Dictionary to store the predictions and targets for each subsample\n",
    "subsample_dict = {}\n",
    "\n",
    "# Get the time range\n",
    "total_length = len(pred_dataset_.time)\n",
    "num_batches_total = math.ceil(total_length / cfg[\"batch_size\"])\n",
    "subsample_indices = np.linspace(num_batches_total/num_subsamples, num_batches_total-1, num=num_subsamples, dtype=int) * 8\n",
    "\n",
    "# Iterate over the dataset for all time steps\n",
    "for i in range(0, len(pred_dataset_.time)):\n",
    "    \n",
    "    # Select the corresponding slices from the prediction and target datasets for the current time step\n",
    "    pred_slice = pred_dataset_.isel(time=i)\n",
    "    targ_slice = target_slice_rolling.isel(time=slice(i, i + 20))\n",
    "\n",
    "    # Convert the data slices to PyTorch tensors and add a batch dimension\n",
    "    pred = torch.tensor(pred_slice.values).unsqueeze(0)  # [1, 20, 51, 120]\n",
    "    targ = torch.tensor(targ_slice.values).unsqueeze(0)  # [1, 20, 51, 120]\n",
    "\n",
    "    # Adjust the grid region if necessary\n",
    "    if adjust_grid_region:\n",
    "        pred = pred[:, :, :, 29:95] if not cfg[\"probabilistic\"] else pred[:, :, :, :, 29:95]\n",
    "        targ = targ[:, :, :, 29:95]\n",
    "\n",
    "    # Store the current predictions and targets in their respective lists\n",
    "    if not cfg[\"probabilistic\"]:\n",
    "        predictions.append((pred, pred_slice.attrs[\"month\"]))\n",
    "    else:\n",
    "        predictions.append((pred[:, 0], pred_slice.attrs[\"month\"]))\n",
    "    \n",
    "    targets.append((targ, target_slice.attrs[\"month\"]))\n",
    "\n",
    "    # When the current time step 'i' is part of the subsample_indices\n",
    "    if i in subsample_indices:\n",
    "        if i == subsample_indices[0]:\n",
    "            del predictions[0]\n",
    "            del targets[0]\n",
    "        # Save the current predictions and targets into the dictionary\n",
    "        subsample_key = f'subsample_{i}'  # Create a unique key for each subsample\n",
    "        subsample_dict[subsample_key] = {\n",
    "            'predictions': predictions.copy(),  # Copy the list to avoid clearing issues\n",
    "            'targets': targets.copy()  # Copy the list to avoid clearing issues\n",
    "        }\n",
    "\n",
    "        # Clear the predictions and targets lists for the next subsample'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================================================\n",
    "# Calculate 3-month rolling mean ACC\n",
    "# ============================================================================================================================\n",
    "\n",
    "'''\n",
    "# New dictionary to store the ACC results for each subsample\n",
    "subsample_dict_acc = {}\n",
    "\n",
    "# Initialize lists to accumulate ACC values over all subsamples\n",
    "total_acc_mean = []\n",
    "total_acc_mean_spatial = []\n",
    "total_acc_mean_3i = []\n",
    "total_acc_mean_3i_spatial = []\n",
    "total_acc_mean_34i = []\n",
    "total_acc_mean_34i_spatial = []\n",
    "total_acc_mean_4i = []\n",
    "total_acc_mean_4i_spatial = []\n",
    "\n",
    "# Initialize counters for lags to accumulate over subsamples\n",
    "num_subsamples = len(subsample_dict)\n",
    "\n",
    "# Iterate over each subsample in subsample_dict\n",
    "for subsample_key, subsample_data in subsample_dict.items():\n",
    "    \n",
    "    # Retrieve predictions and targets from the subsample\n",
    "    predictions = subsample_data['predictions']\n",
    "    targets = subsample_data['targets']\n",
    "\n",
    "    # Initialize lists to store ACC values for this subsample\n",
    "    acc_mean = []\n",
    "    acc_mean_spatial = []\n",
    "    acc_mean_3i = []\n",
    "    acc_mean_3i_spatial = []\n",
    "    acc_mean_34i = []\n",
    "    acc_mean_34i_spatial = []\n",
    "    acc_mean_4i = []\n",
    "    acc_mean_4i_spatial = []\n",
    "\n",
    "    # Iterate over lags (assuming 20 lags)\n",
    "    for lag in range(20):\n",
    "        # Calculate ACC for the entire region\n",
    "        acc = metric.anomaly_correlation_coefficient(predictions, targets, lag, cfg)\n",
    "        acc_mean.append(np.mean(acc))\n",
    "        \n",
    "        acc_spatial = metric.anomaly_correlation_coefficient(predictions, targets, lag, cfg, keep_spatial_coords=True)\n",
    "        stacked_np = np.stack(acc_spatial)\n",
    "        mean_acc_spatial = np.mean(stacked_np, axis=0)\n",
    "        acc_mean_spatial.append(mean_acc_spatial)\n",
    "\n",
    "        # Calculate ACC for Nino 3 region\n",
    "        acc_3i = metric.anomaly_correlation_coefficient(predictions, targets, lag, cfg, nino_index=\"nino_3\")\n",
    "        acc_mean_3i.append(np.mean(acc_3i))\n",
    "\n",
    "        acc_3i_spatial = metric.anomaly_correlation_coefficient(predictions, targets, lag, cfg, nino_index=\"nino_3\", keep_spatial_coords=True)\n",
    "        stacked_3i_np = np.stack(acc_3i_spatial)\n",
    "        mean_acc_3i_spatial = np.mean(stacked_3i_np, axis=0)\n",
    "        acc_mean_3i_spatial.append(mean_acc_3i_spatial)\n",
    "\n",
    "        # Calculate ACC for Nino 3.4 region\n",
    "        acc_34i = metric.anomaly_correlation_coefficient(predictions, targets, lag, cfg, nino_index=\"nino_34\")\n",
    "        acc_mean_34i.append(np.mean(acc_34i))\n",
    "\n",
    "        acc_34i_spatial = metric.anomaly_correlation_coefficient(predictions, targets, lag, cfg, nino_index=\"nino_34\", keep_spatial_coords=True)\n",
    "        stacked_34i_np = np.stack(acc_34i_spatial)\n",
    "        mean_acc_34i_spatial = np.mean(stacked_34i_np, axis=0)\n",
    "        acc_mean_34i_spatial.append(mean_acc_34i_spatial)\n",
    "\n",
    "        # Calculate ACC for Nino 4 region\n",
    "        acc_4i = metric.anomaly_correlation_coefficient(predictions, targets, lag, cfg, nino_index=\"nino_4\")\n",
    "        acc_mean_4i.append(np.mean(acc_4i))\n",
    "\n",
    "        acc_4i_spatial = metric.anomaly_correlation_coefficient(predictions, targets, lag, cfg, nino_index=\"nino_4\", keep_spatial_coords=True)\n",
    "        stacked_4i_np = np.stack(acc_4i_spatial)\n",
    "        mean_acc_4i_spatial = np.mean(stacked_4i_np, axis=0)\n",
    "        acc_mean_4i_spatial.append(mean_acc_4i_spatial)\n",
    "\n",
    "    # Save the ACC values for this subsample into the dictionary\n",
    "    subsample_dict_acc[subsample_key] = {\n",
    "        'acc_mean': acc_mean,\n",
    "        'acc_mean_spatial': acc_mean_spatial,\n",
    "        'acc_mean_3i': acc_mean_3i,\n",
    "        'acc_mean_3i_spatial': acc_mean_3i_spatial,\n",
    "        'acc_mean_34i': acc_mean_34i,\n",
    "        'acc_mean_34i_spatial': acc_mean_34i_spatial,\n",
    "        'acc_mean_4i': acc_mean_4i,\n",
    "        'acc_mean_4i_spatial': acc_mean_4i_spatial\n",
    "    }\n",
    "\n",
    "    # Print the ACC values for this subsample\n",
    "    print(f\"ACC values for subsample {subsample_key}:\")\n",
    "    print(\"Mean ACC: \", acc_mean)\n",
    "    print(\"Mean ACC 3i: \", acc_mean_3i)\n",
    "    print(\"Mean ACC 34i: \", acc_mean_34i)\n",
    "    print(\"Mean ACC 4i: \", acc_mean_4i)\n",
    "    print(\"=====================================================================================================\")\n",
    "\n",
    "    # Accumulate the results for total ACC calculation\n",
    "    if not total_acc_mean:\n",
    "        total_acc_mean = acc_mean\n",
    "        total_acc_mean_spatial = acc_mean_spatial\n",
    "        total_acc_mean_3i = acc_mean_3i\n",
    "        total_acc_mean_3i_spatial = acc_mean_3i_spatial\n",
    "        total_acc_mean_34i = acc_mean_34i\n",
    "        total_acc_mean_34i_spatial = acc_mean_34i_spatial\n",
    "        total_acc_mean_4i = acc_mean_4i\n",
    "        total_acc_mean_4i_spatial = acc_mean_4i_spatial\n",
    "    else:\n",
    "        total_acc_mean = [x + y for x, y in zip(total_acc_mean, acc_mean)]\n",
    "        total_acc_mean_spatial = [x + y for x, y in zip(total_acc_mean_spatial, acc_mean_spatial)]\n",
    "        total_acc_mean_3i = [x + y for x, y in zip(total_acc_mean_3i, acc_mean_3i)]\n",
    "        total_acc_mean_3i_spatial = [x + y for x, y in zip(total_acc_mean_3i_spatial, acc_mean_3i_spatial)]\n",
    "        total_acc_mean_34i = [x + y for x, y in zip(total_acc_mean_34i, acc_mean_34i)]\n",
    "        total_acc_mean_34i_spatial = [x + y for x, y in zip(total_acc_mean_34i_spatial, acc_mean_34i_spatial)]\n",
    "        total_acc_mean_4i = [x + y for x, y in zip(total_acc_mean_4i, acc_mean_4i)]\n",
    "        total_acc_mean_4i_spatial = [x + y for x, y in zip(total_acc_mean_4i_spatial, acc_mean_4i_spatial)]\n",
    "\n",
    "# Calculate the average total ACC over all subsamples\n",
    "total_acc_mean = [x / num_subsamples for x in total_acc_mean]\n",
    "total_acc_mean_spatial = [x / num_subsamples for x in total_acc_mean_spatial]\n",
    "total_acc_mean_3i = [x / num_subsamples for x in total_acc_mean_3i]\n",
    "total_acc_mean_3i_spatial = [x / num_subsamples for x in total_acc_mean_3i_spatial]\n",
    "total_acc_mean_34i = [x / num_subsamples for x in total_acc_mean_34i]\n",
    "total_acc_mean_34i_spatial = [x / num_subsamples for x in total_acc_mean_34i_spatial]\n",
    "total_acc_mean_4i = [x / num_subsamples for x in total_acc_mean_4i]\n",
    "total_acc_mean_4i_spatial = [x / num_subsamples for x in total_acc_mean_4i_spatial]\n",
    "\n",
    "# Save the total ACC values into the dictionary\n",
    "subsample_dict_acc['total'] = {\n",
    "    'acc_mean': total_acc_mean,\n",
    "    'acc_mean_spatial': total_acc_mean_spatial,\n",
    "    'acc_mean_3i': total_acc_mean_3i,\n",
    "    'acc_mean_3i_spatial': total_acc_mean_3i_spatial,\n",
    "    'acc_mean_34i': total_acc_mean_34i,\n",
    "    'acc_mean_34i_spatial': total_acc_mean_34i_spatial,\n",
    "    'acc_mean_4i': total_acc_mean_4i,\n",
    "    'acc_mean_4i_spatial': total_acc_mean_4i_spatial\n",
    "}\n",
    "\n",
    "# Print the final total skill scores\n",
    "print(\"Final Total ACC values:\")\n",
    "for metric_, value in subsample_dict_acc['total'].items():\n",
    "    if 'spatial' not in metric_:\n",
    "        print(f\"{metric_}: {value}\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================================================\n",
    "# Save results\n",
    "# ============================================================================================================================\n",
    "\n",
    "'''\n",
    "results = {\n",
    "        \"dataset\" : dataset,\n",
    "        \"skillscore\" : skillscore_dict,\n",
    "        \"acc\" : subsample_dict_acc,\n",
    "        \"monthly_acc\" : monthly_acc,\n",
    "        \"loss\" : loss_dict,\n",
    "        \"config\": cfg}\n",
    "\n",
    "\n",
    "# Save results to a pickle file\n",
    "with open(f\"C:/Users/felix/PycharmProjects/deeps2a-enso/scripts/evaluation/results/unprocessed/{dataset}/results_{model}_\"+ str(model_num) + \"_\" + run_specification2 + \".pkl\", 'wb') as file:\n",
    "    pickle.dump(results, file) '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeps2aEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
